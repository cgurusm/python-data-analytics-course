{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 09: Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But First..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember from week 7, every data analtics project starts from cleaning the data. The most common steps are:\n",
    "\n",
    "* __Sample the data__ We will need to extract part of the dataset to train our model, often this has to be done where there is a large dataset. \n",
    "* __Impute missing data__ It is quite common that some of the input records are incomplete in the sense that certain fields are missing or have input error. In a typical tabular data format, we need to validate each record contains the same number of fields and each field contains the data type we expect. In case the record has some fields missing, we have the following choices:\n",
    "    * Remove the row if it is imposible to impute them. \n",
    "    * Infer the missing value based on the data from other records. Usually it could be either average, or the median (e.g. if skewed). \n",
    "* __Remove duplicate data__ Clerical errors may arise some rows to have duplicated information. We need to detect them and remove them. \n",
    "* __Inconsistent data__ By business knowledge, experts may inspect which datasets are incorrect. For example they may found incorrect data which values exaggerately large. This is undetectable by computer itself. \n",
    "* __Normalise numeric value__ Transform numeric data into a uniform range is good for training them into machine learning model.\n",
    "* __Reduce dimensionality__ We often want to capture the essense of the dataset, or sometimes it is because the machine learning method prefer simpler datasets. Commonly we can use Principal component analysis (PCA) to do this.\n",
    "* __Add derived features__ In some cases, we may need to compute additional attributes from existing attributes (e.g. linear transform temperature between scales, converting a geolocation to a postcodes). \n",
    "* __Binning numeric value (into categories)__ For numeric attributes, a common way to generalize it is to bin them into ranges, which can be either constant width (variable height/frequency) or variable width (constant height). \n",
    "* __Convert categorical variables into binary variables__ Some machine learning models only take binary input (or numeric input). We may convert the catagorical variables into multiple binary variables (i.e. true and false). Where each label in the catagorical variable is encoded as binary variables for each label.  \n",
    "* __Select, combine, aggregate data__ This includes select which columns are relevant for the model, combine the columns or aggregating multiple raw data records along some dimensions. Often this happens at the end of the EDA and before we finalise which columns should go to our machine learning model. This usually aims to boost our model quality. \n",
    "\n",
    "You must check if the dataset is good for feeding into the machine learning algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application: Classifying Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution": "hidden",
    "solution_first": true
   },
   "source": [
    "__Exercise:__ Texts are unstructure data, then why do we classify this exercise as supervised learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution": "hidden"
   },
   "source": [
    "__Solution:__ The confusion here is that we link unstructure data into unsupervised learning. Truely it is hard to find the target variable from text variables. In this exercise, we entail each messages with a target variable, which is it is positive or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
