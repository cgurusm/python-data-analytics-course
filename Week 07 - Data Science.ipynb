{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7 - Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "solution": "hidden",
    "solution_first": true
   },
   "source": [
    "__Exercise:__ Write a function `hasvowel()` which accepts a word input and returns `True` if the word contains vowels. Think about the requirements of the argument and raise them if incorrect. `string.alpha()` might be helpful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "solution": "hidden"
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "import string   # or from string import isalpha\n",
    "\n",
    "'''\n",
    "If the argument is not a word (i.e. with spaces or non-letter character/ not a string), then raise ValueError. \n",
    "'''\n",
    "def hasvowel(word): \n",
    "    if type(word) != str: \n",
    "        raise ValueError\n",
    "    # Check spaces and non-letter character\n",
    "    if word.isalpha() == False: \n",
    "        raise ValueError\n",
    "    \n",
    "    if ('a' or 'e' or 'i' or 'o' or 'u') in word:\n",
    "        return True   # Otherwise you will obtain False already\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution": "hidden",
    "solution_first": true
   },
   "source": [
    "__Exercise:__ In the following, print out the results (__in 2 decimal points__) using the `.format()` method. The computation is given to you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "solution": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.04\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "from random import randint\n",
    "result = randint(0,122)/100\n",
    "\n",
    "print('{0:.2f}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "result = randint(0,122)/100\n",
    "\n",
    "# Your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution": "hidden",
    "solution_first": true
   },
   "source": [
    "__Exercise:__ Design a pandas dataframe which has the following specification. \n",
    "* `id`: Integers of 8 digits, starting by `10000000`.\n",
    "* `name`: String characters.\n",
    "\n",
    "After this, run your code and show the first 5 records of the dataframe you have created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "solution": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id    name\n",
      "0  10000000  Shelly\n",
      "1  10000001    Kent\n",
      "2  10000002  Martin\n",
      "3  10000003    John\n",
      "4  10000004    Myra\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'id':[10000000,10000001,10000002,10000003,10000004],'name':['Shelly','Kent','Martin','John','Myra']})\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Science Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In data analytics, there is a standard procedure where data scientists follow to commit a data science project. This is called __CRISP-DM__. It follows from generating the ideas, finding the ingredients to produce a data science product and report them. The following is the flowchart of the process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"fig/CRISPDM_Process_Diagram.png\" width=\"620\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url=\"fig/CRISPDM_Process_Diagram.png\", width=620)\n",
    "\n",
    "# Image from https://www.datasciencecentral.com/profiles/blogs/crisp-dm-a-standard-methodology-to-ensure-a-good-outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Your Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first phase, the organisation understands the project objectives and the business requirements. Often they will summarise them in documents. This is the __business understanding__ phase. This is not contrainted to business, any organisation or __you__ have a goal, and this goal is where it guides you to the road of data analytics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise:__ Think about in your career, what is your objective? Think about how this object require data to help you. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To start the data science project, we need to start with the relevant data sources. This is part of the __data understanding__ process. This involves the following steps: \n",
    "* Sourcing data\n",
    "* Exploratory data analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Sourcing Data from Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter is a popular social media site where users can express themselves within some text length. It attracts a lot of attention from researchers to understand how opinion forms on Twitter. To convenient people understand what is inside the social network, Twitter offers an API that allows us to obtain the information in their server. \n",
    "\n",
    "API is the gateway to their servers. This means we need to obtain a key to get access. This is called the __public-key cryptography__. Which means we use the public key to get access into Twitter's server, and Twitter has a private key to encrypt their message back to us. This is only important when we are using the pair of keys as in below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So before to start anything, we should have a Twitter developer account beforehand. This can be obtained from [https://developer.twitter.com/](https://developer.twitter.com/). \n",
    "\n",
    "After that, you will need to obtain your __consumer keys__ and __access tokens__ (i.e. the public keys) on the developer portal. There are many guides available online, for example\n",
    "* [https://towardsdatascience.com/how-to-access-twitters-api-using-tweepy-5a13a206683b](https://towardsdatascience.com/how-to-access-twitters-api-using-tweepy-5a13a206683b)\n",
    "* [https://realpython.com/twitter-bot-python-tweepy/#creating-twitter-api-authentication-credentials](https://realpython.com/twitter-bot-python-tweepy/#creating-twitter-api-authentication-credentials)\n",
    "\n",
    "Once you have created an application, you will need Twitter officially approve you. This can take from hours to few days. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fetch tweets from Twitter, we get help from the `tweepy` package. The following code is would fetch the relevant tweets to the keyword `'pokemon'` (last line of the snippet). \n",
    "```python\n",
    "import tweepy\n",
    "\n",
    "APP_KEY = '***'\n",
    "APP_SECRET = '***'\n",
    "\n",
    "auth = tweepy.OAuthHandler('***', '***')\n",
    "auth.set_access_token(APP_KEY, APP_SECRET)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "tweets = api.search(q = \"pokemon\", count = 100, result_type = \"recent\")\n",
    "```\n",
    "As you can see, we use 2 pairs of keys. The `APP_KEY` represents our access to the server, and then we access the session token which allows us to stay for the time being.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tweepy` provides a lot of methods that could enable programmers to obtain relevant tweets. For example the `search()` method returns relevant tweets from a keyword. `search_users()` returns relevant users from a keyword. \n",
    "\n",
    "More of these methods, and how to use them, can be find in [https://tweepy.readthedocs.io/en/latest/api.html](https://tweepy.readthedocs.io/en/latest/api.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise:__ What does each line of the code mean? If possible, copy the snippet and comment out each lines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In today, we are going to fetch some of the tweets and write them in json format. Json is a way to store complex data in text files and the program can recognise them afterwards. In python there is a native package called `json`, so we will call them at the start. \n",
    "```python\n",
    "import json\n",
    "```\n",
    "To convert the information to json, we write \n",
    "```python\n",
    "tweet_json = json.dump(tweets._json)\n",
    "```\n",
    "Thus we will need to store the data, this means we can write \n",
    "```python\n",
    "with open('tweets.json', 'w') as json_file:\n",
    "    json.dump(tweet_json, json_file)\n",
    "```\n",
    "and the tweet data is now stored in `tweets.json` in your current folder location. \n",
    "\n",
    "To read back the json file, we can use the following snippet: \n",
    "```python\n",
    "with open('data.txt') as json_file:\n",
    "    data = json.load(json_file)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution": "hidden",
    "solution_first": true
   },
   "source": [
    "__Exercise:__ Write a code that will return tweets with the keyword `'Apple'`. Store them into the file called `apple_tweets.txt'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "solution": "hidden"
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "import tweepy\n",
    "\n",
    "APP_KEY = '***'\n",
    "APP_SECRET = '***'\n",
    "\n",
    "auth = tweepy.OAuthHandler('***', '***')\n",
    "auth.set_access_token(APP_KEY, APP_SECRET)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "tweets = api.search(q = \"Apple\", count = 100, result_type = \"recent\")\n",
    "\n",
    "with open('apple_tweets.txt', 'w') as json_file:\n",
    "    json.dump(tweets, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an extension, you can use a Twitter scraper to do more complex scraping. Given that one method call can fetch up to 1500 tweets, you can write a code to find tweets within a date range, and return more than 3200 at once. There are many of these online and often the authors have submitted theirs on thei Github repository. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internet contains a wealth of resources, so we can use python to extract information from the webpage. To scrape from the internet, we follow the following steps: \n",
    "1. Find the page that you want to scrape\n",
    "2. Inspect the page\n",
    "3. Write the code\n",
    "4. Run the code and extract the data\n",
    "5. Store the data in the required format "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will try to scrape from news website. Our example would be from [https://www.nbcnews.com/better/health/what-headline-stress-disorder-do-you-have-it-ncna830141](https://www.nbcnews.com/better/health/what-headline-stress-disorder-do-you-have-it-ncna830141). Which we wish to obtain the article itself. \n",
    "\n",
    "This means we should have a look at the page itself, by means of clicking in the link and inspect them. You should see the similar screenshot as below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"fig/Capture01.png\" width=\"620\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url=\"fig/Capture01.png\", width=620)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see the webapage and it has not only just the news article itself. In well structured websites, there is a top bar to navigate to other pages of the news site. Often there will be some advertisements at the side of the article. If you scroll down, you will see the footer which shows the side information of the website as well. The article does not span the whole space in the middle of the page. There are spaces to put the author's information and the date of publish. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise:__ Identify the page elements in the link. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step is to fetch the webpage onto our computer. In this way, we could potentially extract the information from the page. In this exercise, we will look at a package called `requests`. The following is the snippet of code to extract the website and export the information. \n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "htmlsource = requests.get('https://www.nbcnews.com/better/health/what-headline-stress-disorder-do-you-have-it-ncna830141').text\n",
    "tree = html.fromstring(htmlsource)\n",
    "\n",
    "with open('file/stress_news.html', mode = 'w') as fo:\n",
    "    fo.write(htmlsource)\n",
    "\n",
    "fo.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract the contents, there is a method to do so. Webpages are composed under a markup language called HTML. Each semantic contents are wrapped by a tag. For example, to make text bold, we can write `<b>I am bold.</b>` wich shows __I am bold.__ The bold find starts at the `<b>` tag and ends at `</b>` tag. A typical webpage should show \n",
    "```html\n",
    "<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01//EN\"\n",
    "   \"http://www.w3.org/TR/html4/strict.dtd\">\n",
    "<html>\n",
    "   <head>\n",
    "      <title>My first HTML document</title>\n",
    "   </head>\n",
    "   <body>\n",
    "      <p>Hello world!</p>\n",
    "   </body>\n",
    "</html>\n",
    "```\n",
    "Let us look at the tags in more detail: \n",
    "* The first line tells the computer that this is a webpage. \n",
    "* The `<head>` tag shows the meta information of the webpage. In this case, we define the title of the page under the `<head>` tag. \n",
    "* Anything showed in the `<body>` tag will appear on the screen (of your browser). In this case, there is a sentence 'Hello world!' is written and it is wrapped by the paragraph `<p>` tag. \n",
    "* Finally, every tags should wrap up by a end tag that starts with a `/`. For example, a paragrapha ends with a `</p>` tag. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot assume that our contents are spans within the whole `<body>` tag, so we will inspect it. To do so, at the page opened press __F12__. You should see a portal appeared left to your page with the HTML markups showed. \n",
    "\n",
    "Then scroll to the HTML markups and scroll to the place where the article is highlighting. This is not the place contains only the article, but there is the smallest tag contained the article. So click on the triangle beside the tag, and scroll to where the article is highlighted again. Repeat this process until you are able to see only the article contents are highlighted. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution": "hidden",
    "solution_first": true
   },
   "source": [
    "__Exercise:__ What is the HTML tag that contains the whole article? You may want to include all the information inside the tag. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution": "hidden"
   },
   "source": [
    "It is a `<div>` tag and the whole tag reads \n",
    "```html\n",
    "<div class=\"article-body__content\">\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you read the tag that contains all the information of the article itself, you should see it contains several `<p>` tags, these mean the paragraphs and you don't need to care about them. \n",
    "\n",
    "To extract the article text, the HTML tag we have is important. If you do not have the answer, just use the solution from above exercise. We will extract the article using this information. Given we have the destination tag, how could we tell the computer where it is? We use a concept called __XPath__ (XML Path Language). It shows the address from the top most level to the tag we after. \n",
    "\n",
    "To get the XPath of the article, right click on the tag we after and click copy the 'XPath' or the 'Full XPath'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"fig/Capture02.png\" width=\"480\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url=\"fig/Capture02.png\", width=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the full XPath that stores the article itself is \n",
    "`/html/body/div[2]/div/div[5]/div/div/div/article/div/div[2]/div[1]/div[2]`, while the XPath is `//*[@id=\"content\"]/div/div[5]/div/div/div/article/div/div[2]/div[1]/div[2]`. We can see that `html` appears at the start of the XPath, then the `body`. Which appears to be the nested structure of the html tags. \n",
    "\n",
    "Furthermore, \n",
    "* `/` seperates the different levels. For example, `/html/body/` means the top layer is the `<html>` tag and the `<body>` is nested.\n",
    "* `[]` indicates which branch of the tree is chosen. For instance, if there are 2 `<div>` tags after `<body>`, `div[2]` would select the second (i.e. last) one.\n",
    "* `//` means 'arbitrary depth'. For example, `/html/body//p` means any paragraphs `<p>` under `<body`. Trilling (2019) adivses to 'start your XPATH with `//` to avoid make it shorter and avoid being too specific'.\n",
    "* `*` means 'everything'. For instance, `p[*]` means all paragraphs. \n",
    "* `//*` means everything in the next layer. \n",
    "* `[@attribute=\"whatever\"]` lets you select only those tags that contain a specific attribute. Common attributes could be `id`, `class`. Using above exercise as an example, if the `<div>` tag reads `<div class=\"reviews-single text\">`. The XPath could write \n",
    "`//div[@class=\"article-body__content\"]`. \n",
    "* `.` (a dot) means relative location path from the tag we after. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we can fetch (or _parse_) the article by specifying the XPath. Which is done by the following code snippet. \n",
    "```python\n",
    "from lxml import html\n",
    "\n",
    "article_content = tree.xpath('//*[@id=\"content\"]/div/div[5]/div/div/div/article/div/div[2]/div[1]/div[2]')\n",
    "\n",
    "article_content_text = [r.text.strip() for r in article_content]\n",
    "print(article_content_text)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__Exercise:__ Use the snippets above to print out the texts of the article. If possible, write the texts into a `.txt` file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise:__ Use the snippets above to extract the positive words in the Havard IV-4 dictionary. After that write the list of words into a `.txt` file. \n",
    "\n",
    "This is the link: `http://www.wjh.harvard.edu/~inquirer/Pstv.html`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Some of contents come from https://github.com/info-370/eda._\n",
    "\n",
    "Exploratory data analysis (__EDA__) is the process to understand the datasets sourced. This process starts by summarising the data structures and then visualise a selected features to quickly understand univariate distributions and relationships between variables. Initial EDA questions ask basic questions, including:\n",
    "\n",
    "* How large is the dataset (rows, columns)?\n",
    "* What are the variables present in the dataset?\n",
    "* What is the data type of each variable?\n",
    "\n",
    "In this week, we will start some work on EDA with a marks dataset and a text dataset. You can source your own dataset as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using Jupyter to analyse data, we should maintain our habit on grouping each functions into each cell. For example, a conventional layout should include (in order): \n",
    "1. Packages/ libraries needed to be imported\n",
    "2. Custom functions you wish to use\n",
    "3. Import data\n",
    "4. EDA\n",
    "5. Analysis\n",
    "6. Report on your findings\n",
    "\n",
    "In this way it is organised, and also it is intuitive to later users to compile the early cells first so that there will be no not found errors. In the following, we have a dedicated cell for storing the packages. When we need to import a library, we will add into the next cell and rerun it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import all needed packages in here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will use a sample marks data set called `marks.csv`. It is stored in the `/files` folder. It is generated dataset where it simulates the breakdown of marks in a course. There are several columns: \n",
    "* Unikey is a unique identifier of each students. \n",
    "* Final exam marks (by question, worth 50%): \n",
    "    * `'Q1'` is out of 20.\n",
    "    * `'Q2'` is out of 20.\n",
    "    * `'Q3'` is out of 15.\n",
    "    * `'Q4'` is out of 20.\n",
    "    * `'Q5'` is out of 15.\n",
    "    * `'Q6'` is out of 10.\n",
    "* `'Quizes'` worth 10% of the course. \n",
    "* `'Lab'` worth 10% of the course. \n",
    "* `'Asgn 01'` worth 10% of the course. \n",
    "* `'Asgn 02'` worth 20% of the course. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exericse:__ Read the `marks.csv` file and import the data as a `pandas` dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code below\n",
    "df = pd.read_csv('???')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the input `.csv` file is encoded by different systems. For example, if the `.csv` file is written in Chinese or other languages, it may use unknown encoding systems to us. Sometimes, we need to add an option to the `.read_csv()` to let it read the Unicode. For example, \n",
    "\n",
    "```python\n",
    "df = pd.read_csv('???', encoding='utf8')\n",
    "```\n",
    "\n",
    "If you see the encoding error and you cannot figure out which encoding system it uses. Run the following code and it will help you. Then use the code snippet above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This code checks what encoding the csv file has. \n",
    "with open('top50.csv') as f:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we don't need to worry about the encoding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let us start exploring the data set. This means we need to print out the features inside the dataset. In the following, we are using the methods from `pandas` since last week. Which would be familiar to you. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution": "hidden",
    "solution_first": true
   },
   "source": [
    "__Exercise:__ How can we see the first __10 records__ of the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "solution": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     unikey    Q1    Q2    Q3    Q4    Q5    Q6  Quizes     Lab Asgn 01  \\\n",
      "0  pmpk3028   6.0   8.5   2.5   3.0   7.0   5.0   3.47%   3.27%  84.27%   \n",
      "1  iyqw3940  16.5  18.5   1.0  14.0  14.5   8.0  14.08%  91.72%  56.11%   \n",
      "2  mjll6065  11.5  11.5   0.5  14.5  11.0   0.0  84.66%   75.5%   48.3%   \n",
      "3  rtgm8060   6.0  14.5   5.0   8.0   7.0   0.0  59.99%   52.2%  19.31%   \n",
      "4  xfxd8275  12.0   4.0   1.5   3.5  14.5   6.0  23.46%  78.64%   2.32%   \n",
      "5  usan5260   1.5  18.0   4.5   7.5   8.0   2.5  11.53%  33.07%  53.54%   \n",
      "6  exgv5076   5.5   7.0   2.0  13.5  10.0   9.5  21.71%   1.31%   56.8%   \n",
      "7  dprj4333   1.5  20.5  10.0  13.0   1.5  10.0  76.35%  41.73%  27.52%   \n",
      "8  vera2617   4.5  15.0  11.0   1.0   5.5  10.5   69.2%  13.23%  30.51%   \n",
      "9  fuea3500   4.5  13.5   5.0   1.5   4.5   6.5  74.32%  96.75%  87.45%   \n",
      "\n",
      "  Asgn 02  \n",
      "0  50.17%  \n",
      "1   8.78%  \n",
      "2  98.73%  \n",
      "3  90.93%  \n",
      "4  91.49%  \n",
      "5  49.17%  \n",
      "6  89.46%  \n",
      "7  84.06%  \n",
      "8  30.62%  \n",
      "9  13.52%  \n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution": "hidden",
    "solution_first": true
   },
   "source": [
    "__Exercise:__ How can we know what columns are in the data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "solution": "hidden"
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution": "hidden",
    "solution_first": true
   },
   "source": [
    "__Exercise:__ How can we find out how many rows and columns we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "solution": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 11)\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution": "hidden",
    "solution_first": true
   },
   "source": [
    "__Exericse:__ Given that we know how to find how many rows and columns we have in teh dataset? I would like to see the results printed as `'There are {} records and {} columns in this dataset.'` where the brackets are substituted by the number of rows and columns in the dataset. \n",
    "\n",
    "__Hint:__ Use the code above and think about how come the returned value able to fit in 2 values. What is the data type we obtained? So how can we extract the individual values? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "solution": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 250 records and 11 columns in this dataset.\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "print('There are {} records and {} columns in this dataset.'.format(df.shape[0],df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution": "hidden",
    "solution_first": true
   },
   "source": [
    "__Exercise:__ How can we find out the data types under each column? Are they all the same under each columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "solution": "hidden"
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of your EDA, we will start to get some statistical information from the dataset. To do so, we rely on the native `.describe()` method from `pandas`. Run the code below to see how the amrks distribution would be. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let us start with the results row by row. \n",
    "* The first column tells us how many (non null) records do we have in this dataset. Given that all counts are the same, we can say there are no missing (null) values. However, we can check that from the `.shape()` method previously. \n",
    "* The next 2 rows represents the mean and standard deviation of each questions. It would be meaningful to interpret these results given we have the total marks of each question. \n",
    "* The next rows represent their quantiles. That is, for example, how does the lowest 25% perform? We can have a percetion on where the cohort perform. Are they doing good in this question, or not?\n",
    "\n",
    "However, if we look at the last row. There is something wrong in here. If you look carefully, you should see that the maximum marks are 0.5 more than what supposed to be. We need to find out how many people have this mark and then rectify the situation. This will be done in the next subsection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It also involves data cleaning means to find out the outliers, for example \n",
    "* Which data points has value too big?\n",
    "* Which data points are ilogical?\n",
    "* How many missing data do we have?\n",
    "\n",
    "Often this is done once we have a dataset. For the purose of this week, it is done in the next step. In pratice, data cleaning has to be done when we obtain a new piece of dataset. In the following, let us use some of the templates to clean the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From previously, we have found out that few people who had more marks than expected. To find out the how many people have this problem, we use the following code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('=== Excessive marks awarded ===')\n",
    "print('Q1:',df.loc[df.Q1 == 20.5, 'Q1'].count(), 'students')\n",
    "print('Q2:',df.loc[df.Q2 == 20.5, 'Q2'].count(), 'students')\n",
    "print('Q3:',df.loc[df.Q3 == 15.5, 'Q3'].count(), 'students')\n",
    "print('Q4:',df.loc[df.Q4 == 20.5, 'Q4'].count(), 'students')\n",
    "print('Q5:',df.loc[df.Q5 == 15.5, 'Q5'].count(), 'students')\n",
    "print('Q6:',df.loc[df.Q6 == 10.5, 'Q6'].count(), 'students')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you able to diagnose the problem? Think about a cause of this incorrect data entry and write a code to see why. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason why this happens is becuase when this dataset was generated, it was based on a code that aimed to generate `0.5` marks. However, this will cause the the problem above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that not many people have this problem, that is fine. For the purpose of this exercise, let us convert the marks into full marks. This is the code to do that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.loc[df.Q1 == 20.5, 'Q1'] = 20\n",
    "df.loc[df.Q2 == 20.5, 'Q2'] = 20\n",
    "df.loc[df.Q3 == 15.5, 'Q3'] = 15\n",
    "df.loc[df.Q4 == 20.5, 'Q4'] = 20\n",
    "df.loc[df.Q5 == 15.5, 'Q5'] = 15\n",
    "df.loc[df.Q6 == 10.5, 'Q6'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you explain what does the code mean? You might need to see the `.loc` documentations in `pandas`. \n",
    "\n",
    "So now we have corrected the illogical data, and let us see how is the dataframe behaving. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run it\n",
    "print('=== Excessive marks awarded ===')\n",
    "print('Q1:',df.loc[df.Q1 == 20.5, 'Q1'].count(), 'students')\n",
    "print('Q2:',df.loc[df.Q2 == 20.5, 'Q2'].count(), 'students')\n",
    "print('Q3:',df.loc[df.Q3 == 15.5, 'Q3'].count(), 'students')\n",
    "print('Q4:',df.loc[df.Q4 == 20.5, 'Q4'].count(), 'students')\n",
    "print('Q5:',df.loc[df.Q5 == 15.5, 'Q5'].count(), 'students')\n",
    "print('Q6:',df.loc[df.Q6 == 10.5, 'Q6'].count(), 'students')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another illogical data occured in the dataset. If you look carefully, the assessment marks (rightmost of the columns) are strings. These are stored as percentages and in strings. So we will need to carefully convert the pecentage into numbers by \n",
    "1. Strip the percentage sign away before we convert the data\n",
    "2. Convert the data points using `.astype()`. \n",
    "\n",
    "So let us convert the columns in below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Complete the code and run it\n",
    "df['Quizes Marks'] = df['Quizes'].str.slice(0,-1)\n",
    "df['Quizes Marks'] = df['Quizes Marks'].astype(float)\n",
    "df['Quizes Marks']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reality, data is rarely clean and homogeneous. In particular, realistic datasets will have some amount of data missing. We will discuss how to treat missing data, discuss how `pandas` chooses to represent it. There are 2 ways in Python to represent missing data, either: \n",
    "* `None`, or \n",
    "* `numpy.nan` (acronym for _Not a Number_). \n",
    "The first one we know it from native Python coding, while the second one comes from `numpy`. They are both equivalent when we use a dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us do an exercise about missing values. In the following, we use the marks database and, there are few students' marks that were added laterly. Run the code below to get a glimse of the new dataset (_Remember viewing the dataset itself it an important part of EDA_). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run it\n",
    "new_data = pd.DataFrame([['qbis1318',15,18,0.5,0,0,0,'92.50%','100.00%','100.00%',None],['bzhe1028',3,5,0,0,7,0,'32.18%','0.00%','100.00%',None],['azha9208',2,0,5,0,0,0,None,None,None,None],['jwaj9208',5,8,11.5,5,1,0,'50.55%','25.18%','100.00%',None],['tbai9208',5,8,11.5,5,1,0,'72.63%',None,'100.00%',None]], columns=df.columns) \n",
    "print(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing data points occured because these students have few issues with administration. For the purpose of this exercise, we just work on the missing value through means of data analysis. Now let us add the dataset into the original one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df, new_data], ignore_index=True)\n",
    "print(df.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we should do is to detect how many missing values we have. In `pandas`, we have a method called `.isnull()`. Either one will return a Boolean mask over the data. For example, \n",
    "```python \n",
    "df2 = pd.DataFrame([[0.5,0,2,Null]])\n",
    "df2.isnull()\n",
    "```\n",
    "will return \n",
    "\n",
    "```\n",
    "0    False\n",
    "1    False\n",
    "2    False\n",
    "3    True\n",
    "dtype: bool\n",
    "```\n",
    "In the following, the code for inspecting how many null values are partially prepared. It is to count how many null values under each column. Think about what each method means and how they are applied in here. Complete the cell below to inspect all columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('=== Null values ===')\n",
    "print(df['unikey'].isnull().sum())\n",
    "print(df['Q1'].isnull().sum())\n",
    "print(df['Q2'].isnull().sum())\n",
    "print(df['Q3'].isnull().sum())\n",
    "print(df['Q4'].isnull().sum())\n",
    "print(df['Q5'].isnull().sum())\n",
    "print(df['Quizes'].isnull().sum())\n",
    "# Continue the code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biggest problem of having missing data is therefore we cannot compute further. Often `pandas` methods do not like missing data and we will see an exception. So we will need to eliminate missing data. There are some operations still allow missing values, however, they will be \"eaten\" by the missing value. For example, try the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run this\n",
    "1 + np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run this\n",
    "0 * np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see that arithmetic operations are absorbed by the missing data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By rule of thumb, if the amount of missing data is less than 10%, then we should not be too worried of dropping them. In this exercise, we will not look at dropping rows too further. In `pandas`, the method to drop missing data is the `.dropna()`. For example, \n",
    "```python\n",
    "df.dropna()\n",
    "```\n",
    "will __drop all the rows or columns__ that contains missing value. This will lead back to the original dataframe we have. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other option is to fill in the missing values. This will be the approach we pursue this time, so that each student will have a mark back. In `pandas`, the `.fillna()` method suits the needs. There are different approaches to do, here are the common appraoches: \n",
    "* Fill with `0`\n",
    "* Fill using mean/ median value\n",
    "* Bring in value from previous row (_forward-fill_)\n",
    "* Bring in value from next row (_back-fill_)\n",
    "* Based on values from the other column(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we will fill in the missing values will `0`. Which is, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try this\n",
    "df.fillnan(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have explored most of the data set. We will start find the correlations between different datasets. \n",
    "```python\n",
    "import seaborn as sns\n",
    "sns.set(style=\"ticks\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA with Text Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these 2 weeks we will also look at how to infer from text files. The key of analysing text data sets is to encode them quantitatively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0\n",
      "0  As devastating stories about the migrant child...\n",
      "1  Wading through the flood of headlines, I feel ...\n",
      "2  “The last few days I’ve had [sessions] with pe...\n",
      "3  Though it can feel like we’re giving up on a m...\n",
      "4  A study by the American Psychological Associat...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "news = pd.read_csv('files/news.txt', sep='\\n', header=None)\n",
    "print(news.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "news[0].str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9b0e5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.bar(list(range(news[0].shape[0])), news[0].str.len())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise:__ In the plot above, add a meaningful header for it. What is it about?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run it\n",
    "from nltk.sentiment import vader\n",
    "senti=vader.SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.455, 'pos': 0.545, 'compound': 0.6989}\n",
      "{'neg': 0.513, 'neu': 0.487, 'pos': 0.0, 'compound': -0.2755}\n",
      "{'neg': 0.183, 'neu': 0.687, 'pos': 0.13, 'compound': -0.1938}\n"
     ]
    }
   ],
   "source": [
    "# Try yourself\n",
    "print(senti.polarity_scores('I really love what I\\'ve got!'))\n",
    "print(senti.polarity_scores(\"I don't like this\"))\n",
    "print(senti.polarity_scores(\"I liked the course, but I don't like the bits and pieces of it.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     {'neg': 0.09, 'neu': 0.91, 'pos': 0.0, 'compou...\n",
      "1     {'neg': 0.134, 'neu': 0.839, 'pos': 0.027, 'co...\n",
      "2     {'neg': 0.121, 'neu': 0.779, 'pos': 0.1, 'comp...\n",
      "3     {'neg': 0.076, 'neu': 0.801, 'pos': 0.123, 'co...\n",
      "4     {'neg': 0.093, 'neu': 0.907, 'pos': 0.0, 'comp...\n",
      "5     {'neg': 0.136, 'neu': 0.792, 'pos': 0.072, 'co...\n",
      "6     {'neg': 0.506, 'neu': 0.494, 'pos': 0.0, 'comp...\n",
      "7     {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...\n",
      "8     {'neg': 0.047, 'neu': 0.843, 'pos': 0.11, 'com...\n",
      "9     {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...\n",
      "10    {'neg': 0.119, 'neu': 0.761, 'pos': 0.12, 'com...\n",
      "11    {'neg': 0.0, 'neu': 0.707, 'pos': 0.293, 'comp...\n",
      "12    {'neg': 0.133, 'neu': 0.621, 'pos': 0.246, 'co...\n",
      "13    {'neg': 0.0, 'neu': 0.734, 'pos': 0.266, 'comp...\n",
      "14    {'neg': 0.122, 'neu': 0.828, 'pos': 0.05, 'com...\n",
      "15    {'neg': 0.0, 'neu': 0.894, 'pos': 0.106, 'comp...\n",
      "16    {'neg': 0.061, 'neu': 0.825, 'pos': 0.115, 'co...\n",
      "17    {'neg': 0.423, 'neu': 0.577, 'pos': 0.0, 'comp...\n",
      "18    {'neg': 0.037, 'neu': 0.857, 'pos': 0.106, 'co...\n",
      "19    {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...\n",
      "20    {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...\n",
      "21    {'neg': 0.148, 'neu': 0.666, 'pos': 0.187, 'co...\n",
      "22    {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...\n",
      "23    {'neg': 0.142, 'neu': 0.771, 'pos': 0.087, 'co...\n",
      "24    {'neg': 0.344, 'neu': 0.656, 'pos': 0.0, 'comp...\n",
      "25    {'neg': 0.021, 'neu': 0.794, 'pos': 0.185, 'co...\n",
      "26    {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...\n",
      "27    {'neg': 0.199, 'neu': 0.731, 'pos': 0.07, 'com...\n",
      "28    {'neg': 0.068, 'neu': 0.899, 'pos': 0.033, 'co...\n",
      "29    {'neg': 0.171, 'neu': 0.829, 'pos': 0.0, 'comp...\n",
      "30    {'neg': 0.0, 'neu': 0.565, 'pos': 0.435, 'comp...\n",
      "31    {'neg': 0.043, 'neu': 0.814, 'pos': 0.142, 'co...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "ps = news[0].apply(senti.polarity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps[0]['neg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In data science, understanding data initiates the project. It is important to foresee how the data will be important to the deliverables. In the next process __data preparation__, it involves with the following processes: \n",
    "* Deriving new columns\n",
    "* Merging data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the goal of data analysis is to explain something. Therefore, we need either __a target feature__ to explain or any quantitative features (e.g. how close the data points/ clusters). In this section, we will derive the marks column and infer from the that. Note that we will not merge data sources as we are working on only 1 data source at this point. \n",
    "\n",
    "To create a new column, we simply assign values to a column we have never seen from the data source. In particular, the marks breakdown of the course are: \n",
    "* Final exam marks  worth 50%of the course. \n",
    "* Quizes worth 10% of the course. \n",
    "* Lab worth 10% of the course. \n",
    "* Assignment 01 worth 10% of the course. \n",
    "* Assignment 02 worth 20% of the course. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution": "hidden",
    "solution_first": true
   },
   "source": [
    "__Exercise:__ Create the `'Mark'` column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "solution": "hidden"
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "df['Mark'] = (df['Q1'] + df['Q2'] + df['Q3'] + df['Q4'] + df['Q5'] + df['Q6'])/2 + df['Quizes Marks'] * 0.1 + df['Lab Marks'] * 0.1 + df['Asgn 01 Marks'] * 0.1 + df['Asgn 02 Marks'] * 0.2\n",
    "df['Mark']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code below\n",
    "\n",
    "df['Mark']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can visuallise the data points using `matplotlib` library. It is a Python library that presents our data visually. To call `matplotlib`, simply use the following line into our cell for calling libraries above\n",
    "```python\n",
    "from matplotlib import pyplot as plt\n",
    "% matplotlib inline\n",
    "```\n",
    "You can see that we are importing a library `pyplot` from `matplotlib`, which is commonly used in data analytics. We then shortform the library called `plt`. The second line is unique line to Jupyter, which is to enable the plot outputs show and store in this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The usual syntax of plotting data is the following: \n",
    "```python\n",
    "plt.plot(data)\n",
    "plt.xlabel('x')\n",
    "plt.show()  # In Jupyter, it can be omitted if inline function used\n",
    "```\n",
    "Each lines above represents each part to plot a diagram. The __first line__ tells us what kind of plots to you want. `pyplot` provides several options to plot (e.g. box plot `plt.box()`, scatter plot `plt.scatter()`, line plot `plt.plot()`) data. The __second line__ is optinally for the bit and pieces (e.g. your axes label, how big is your plot) if your plot in the previous line. __Finally__, use `plt.show()` to display your work. \n",
    "\n",
    "There are plenty of exercises from their [official website](https://matplotlib.org/gallery/index.html). Click on one of the examples and there are many ideas come from the codes underneath the display. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution": "hidden",
    "solution_first": true
   },
   "source": [
    "Let us do an example with `pyplot`, let us plot with a sample data. \n",
    "\n",
    "__Exercise:__ Complete the code below to plot a line plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "solution": "hidden"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlBiN+xnACQQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9d72550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Solution\n",
    "from matplotlib import pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "plt_data = np.array([1,0,1,0,5,6,8,10,12,15,19])\n",
    "plt.plot(plt_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt_data = np.array[1,0,1,0,5,6,8,10,12,15,19]\n",
    "# Your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution": "hidden",
    "solution_first": true
   },
   "source": [
    "So now let us plot the marks of each student in the marks dataset. \n",
    "\n",
    "__Exercise:__ Plot the marks of each student using a scatter plot. What do the scatter plot mean? (Use `plt.scatter()` to help you)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "solution": "hidden"
   },
   "outputs": [],
   "source": [
    "# Solution \n",
    "plt.plot(df['Mark'])\n",
    "plt.show()\n",
    "print('The scatter shows marks of each student, in the order of the dataframe rows.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let us plot the distribution of people who passed and failed the course. The following is the demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run it \n",
    "plt.plot(df.loc[df.Mark >= 50, 'Mark'], 'bo')\n",
    "plt.plot(df.loc[df.Mark < 50, 'Mark'], 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you explain what is in each line of code above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "After all the preparation work has done, it is time to plan to make the deliverables. In data analytics, we make __models__. These are the mathematical models. You can think them as a line plot in front of the computer screen. A model is how we conceptualise a concept in a consise way. For example, \n",
    "* We describe how the population of birds grow in a nature reserve through a line plot through times.\n",
    "* After we find out why the sewage system blocked in the whole city, we could draw a map of the sewage system to show the situation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So once we have our datasets on hand, we can start modelling. There are many ways to model from data: \n",
    "* Statistical modelling (See __next week__) \n",
    "* Machine learning (See __week 9 and 10__) \n",
    "* Dynamical equation modelling\n",
    "* Agent based modelling\n",
    "\n",
    "Usually a statistical modelling means we find out how does a feature of the target we measure associate with another feature. So we draw a line diagram from it. For machine learning, it depends on your choice and you can check more from [Google](https://developers.google.com/machine-learning/problem-framing/cases). More often, you don't need to use machine learning to model your dataset. There is a worksheet from Google to help you whether you need machine learning or not. \n",
    "\n",
    "[https://developers.google.com/machine-learning/problem-framing/framing](https://developers.google.com/machine-learning/problem-framing/framing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of your modelling process. You will need to evaluate your model. This will be mentioned more next week upon the technical sides. Often you will need to measure your model with __model diagnostics__. Therefore, you will able to ask yourself the following questions: \n",
    "* Does my model contribute to what I want?\n",
    "* Is there any way to optimise my computation?\n",
    "* Any surprises (both good and bad) occured when modelling?\n",
    "* Any mistakes and lessons learnt?\n",
    "\n",
    "Think about these questions once we have done next week's work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of your data analytics cycle, you will be writing summary about your model and then use it in real life. There are many examples: \n",
    "* Recommendation systems on Youtube\n",
    "* Weather prediction \n",
    "* Spread of bushfires\n",
    "\n",
    "While each dataset is unique, there is always possibilities to its applications. \n",
    "\n",
    "__Exercise:__ Think about what could be the applications from the data analysis we have just made. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you can't think of any applications on top of your head, that is fine. There are many ways to think about how to produce a useful application out of them. We call them as __deliverables__. In reality, people will: \n",
    "1. Summarise the results from their models\n",
    "2. Create a step-by-step plan for deployment\n",
    "3. You may then also wanted to see: \n",
    "    * Any factors or influences needed to follow up? \n",
    "    * Validity and accuracy of each model\n",
    "    * Does the model become unrealistic?\n",
    "\n",
    "After that you will need to write a final report to summarise your cycle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we finish, do you remember that in the CRISP-DM flowchart, what does the arrows look like? In fact, any business process is never uni-directional. Always remember to review your work. It will help you to detect any mistakes early as possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this week, we have looked into the process of data analytics and \n",
    "* How to define a solid purpose for a data science project\n",
    "* How to complete EDA\n",
    "* Understand data analytics process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Further Reading\n",
    "\n",
    "* Trilling, D. (2019). _Doing Computational Social Science with Python: An Introduction_. [Online; Accessed on 16th December 2019]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
